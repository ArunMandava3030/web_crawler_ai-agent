# ğŸ•·ï¸ AI Web Crawler + RAG Pipeline

This project is an **AI-powered web crawler and knowledge extraction pipeline** that can crawl websites, extract clean text, embed it using **Qwen embeddings**, store it in **ChromaDB**, and perform **Retrieval-Augmented Generation (RAG)** to answer user questions.  

It also includes a **Streamlit frontend** where users can:
- Crawl and extract structured knowledge from a website  
- Store knowledge in a vector database  
- Ask questions and get AI-powered answers  
- Download the extracted knowledge as a JSON file  

---

## ğŸš€ Features

- ğŸŒ **Website Crawling** â†’ Powered by [Crawl4AI](https://github.com/unclecode/crawl4ai)  
- ğŸ“„ **Clean Text Extraction** â†’ Extracts readable content instead of raw HTML  
- ğŸ§  **Embeddings with Qwen** â†’ Uses Alibabaâ€™s Qwen embedding API  
- ğŸ—‚ï¸ **Vector Store with ChromaDB** â†’ Stores embeddings for efficient similarity search  
- ğŸ” **RAG Question Answering** â†’ Retrieves relevant chunks + generates answers  
- ğŸ’¾ **Export JSON** â†’ Download extracted website knowledge before embedding  
- ğŸ–¥ï¸ **Streamlit UI** â†’ Simple interactive frontend  

---

## ğŸ“‚ Project Structure

```

project\_root/
â”‚â”€â”€ scripts/
â”‚   â”œâ”€â”€ run\_pipeline.py      # CLI pipeline runner
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ streamlit\_app.py     # Streamlit frontend
â”‚â”€â”€ core/
â”‚   â”œâ”€â”€ crawler.py           # Crawl4AI-based crawler
â”‚   â”œâ”€â”€ embedder.py          # Qwen embedding functions
â”‚   â”œâ”€â”€ vector\_store.py      # ChromaDB vector store
â”‚   â”œâ”€â”€ rag.py               # Retrieval + QA logic
â”‚â”€â”€ outputs/
â”‚   â”œâ”€â”€ extracted\_data.json  # Sample extracted JSON
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md

````

---

## ğŸ”§ Installation

1. **Clone the repo**
   ```bash
   git clone https://github.com/your-username/ai-web-crawler-rag.git
   cd ai-web-crawler-rag
````

2. **Create virtual environment**

   ```bash
   python -m venv venv
   source venv/bin/activate   # Mac/Linux
   venv\Scripts\activate      # Windows
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

4. **Set environment variables**
   Create a `.env` file in the project root:

   ```ini
   QWEN_API_KEY=your_api_key_here
   ```

---

## â–¶ï¸ Usage

### 1. Run pipeline from CLI

Crawl a website, embed it, and store in ChromaDB:

```bash
python scripts/run_pipeline.py https://www.chrismytton.com/plain-text-websites/
```

### 2. Run Streamlit frontend

Launch the interactive dashboard:

```bash
streamlit run app/streamlit_app.py
```

---

## ğŸ§© Example Workflow

1. Enter a website URL in the Streamlit UI
2. The system crawls and extracts text
3. JSON output is generated â†’ downloadable before embedding
4. Embeddings are created with Qwen
5. ChromaDB stores vectors
6. Ask questions â†’ Answers are generated with RAG

---

## ğŸ“¸ Screenshots (Optional)

*Add screenshots of the Streamlit app UI, crawling process, and Q\&A here.*

---

## âš¡ Tech Stack

* [Python 3.10+](https://www.python.org/)
* [Crawl4AI](https://github.com/unclecode/crawl4ai) - Web crawling
* [Qwen API](https://dashscope.aliyuncs.com/) - Embeddings
* [ChromaDB](https://www.trychroma.com/) - Vector database
* [Streamlit](https://streamlit.io/) - Frontend

---

## ğŸ› ï¸ Troubleshooting

* **Crawl4AI fails** â†’ Check if site blocks bots (use headers or delays).
* **Qwen API 404** â†’ Make sure the embedding endpoint is correct and API key is valid.
* **FAISS/Chroma dimension mismatch** â†’ Ensure embeddings are stored with consistent dimensions.
* **Empty JSON output** â†’ Site may have dynamic JS content. Use a different crawler config.

---

## ğŸ“œ License

MIT License. See [LICENSE](LICENSE) for details.

---

## ğŸ‘¨â€ğŸ’» Author

**Arun Mandava**

* ğŸŒ [LinkedIn](https://linkedin.com/in/arun-mandava)
* ğŸ’» Data Scientist | AI & Web Crawling Enthusiast

